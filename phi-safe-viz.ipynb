{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file phi1_5_updated/config.json\n",
      "Model config PhiConfig {\n",
      "  \"_name_or_path\": \"phi1_5_updated\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": null,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": null,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"phi\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"partial_rotary_factor\": 0.5,\n",
      "  \"qk_layernorm\": false,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1880\n",
      "}\n",
      "\n",
      "loading weights file phi1_5_updated/model.safetensors\n",
      "Generate config GenerationConfig {}\n",
      "\n",
      "All model checkpoint weights were used when initializing PhiForCausalLM.\n",
      "\n",
      "All the weights of PhiForCausalLM were initialized from the model checkpoint at phi1_5_updated.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n",
      "loading configuration file phi1_5_updated/generation_config.json\n",
      "Generate config GenerationConfig {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [N5P][Th][N3[Sc+2][Th]%1B%47[Br-]2+][Br-][Br-]g+][14[Th][N3[17F][Th][N5[Ni-4][Br-]2+][Br-][Br-][N5P][Th][N3B[Br-][Br-][N4P][Th][N5[Sc+2][Th]%1\n"
     ]
    },
    {
     "ename": "SAFEDecodeError",
     "evalue": "Failed to decode [N5P][Th][N3[Sc+2][Th]%1B%47[Br-]2+][Br-][Br-]g+][14[Th][N3[17F][Th][N5[Ni-4][Br-]2+][Br-][Br-][N5P][Th][N3B[Br-][Br-][N4P][Th][N5[Sc+2][Th]%1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/safemol/lib/python3.11/site-packages/safe/converter.py:434\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(safe_str, as_mol, canonical, fix, remove_added_hs, remove_dummies, ignore_errors)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[43msafe_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mas_mol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_mol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcanonical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_dummies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_dummies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_added_hs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_added_hs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/safemol/lib/python3.11/site-packages/safe/converter.py:187\u001b[0m, in \u001b[0;36mSAFEConverter.decoder\u001b[0;34m(self, inp, as_mol, canonical, fix, remove_dummies, remove_added_hs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical:\n\u001b[0;32m--> 187\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_smiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/safemol/lib/python3.11/site-packages/datamol/mol.py:409\u001b[0m, in \u001b[0;36mstandardize_smiles\u001b[0;34m(smiles)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mApply smile standardization procedure. This is a convenient function wrapped arrounf RDKit\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03msmiles standardizer and tautomeric canonicalization.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    standard_smiles: the standardized smiles\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m smiles \u001b[38;5;241m=\u001b[39m \u001b[43mrdMolStandardize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStandardizeSmiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m smiles\n",
      "\u001b[0;31mArgumentError\u001b[0m: Python argument types in\n    rdkit.Chem.MolStandardize.rdMolStandardize.StandardizeSmiles(NoneType)\ndid not match C++ signature:\n    StandardizeSmiles(std::string smiles)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSAFEDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_sequences[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_text)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMILES: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43msafe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m )\n",
      "File \u001b[0;32m~/anaconda3/envs/safemol/lib/python3.11/site-packages/safe/converter.py:446\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(safe_str, as_mol, canonical, fix, remove_added_hs, remove_dummies, ignore_errors)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ignore_errors:\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SAFEDecodeError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "\u001b[0;31mSAFEDecodeError\u001b[0m: Failed to decode [N5P][Th][N3[Sc+2][Th]%1B%47[Br-]2+][Br-][Br-]g+][14[Th][N3[17F][Th][N5[Ni-4][Br-]2+][Br-][Br-][N5P][Th][N3B[Br-][Br-][N4P][Th][N5[Sc+2][Th]%1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import safe\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_id = \"./.saved_model/phi1_5-safmol\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id+\"/checkpoint-100\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define generation parameters\n",
    "generation_args = {\n",
    "    \"max_length\": 50,  # Adjust the maximum length of the generated sequence\n",
    "    \"num_return_sequences\": 1,  # Number of sequences to generate\n",
    "    \"temperature\": 1.0,  # Sampling temperature\n",
    "    \"top_k\": 50,  # Top-k sampling\n",
    "    \"top_p\": 0.95,  # Top-p (nucleus) sampling\n",
    "    \"do_sample\": True,  # Use sampling instead of greedy decoding\n",
    "}\n",
    "\n",
    "# Generate a sequence without a prompt\n",
    "input_ids = torch.tensor([[tokenizer.pad_token_id]])  # Use the pad token ID as a dummy input\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_sequences = model.generate(input_ids, **generation_args)\n",
    "\n",
    "# Decode the generated sequence\n",
    "generated_text = tokenizer.decode(generated_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "print(\"SMILES: \", safe.decode(generated_text, canonical=True) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safemol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
