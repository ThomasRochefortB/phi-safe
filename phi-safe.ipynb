{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', '1', '2', 'c', 'c', 'c', '3', 'c', 'c', '1', '.', 'C', '3', '(', 'C', ')', 'C', '(', '=', 'O', ')', 'O', '.', 'C', 'C', '(', 'C', ')', 'C', '2']\n"
     ]
    }
   ],
   "source": [
    "import safe\n",
    "\n",
    "ibuprofen = \"CC(Cc1ccc(cc1)C(C(=O)O)C)C\"\n",
    "\n",
    "# SMILES -> SAFE -> SMILES translation\n",
    "try:\n",
    "    ibuprofen_sf = safe.encode(ibuprofen)  # c12ccc3cc1.C3(C)C(=O)O.CC(C)C2\n",
    "    ibuprofen_smi = safe.decode(ibuprofen_sf, canonical=True)  # CC(C)Cc1ccc(C(C)C(=O)O)cc1\n",
    "except safe.EncoderError:\n",
    "    pass\n",
    "except safe.DecoderError:\n",
    "    pass\n",
    "\n",
    "ibuprofen_tokens = list(safe.split(ibuprofen_sf))\n",
    "print(ibuprofen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:rdkit=\"http://www.rdkit.org/xml\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.1\" baseProfile=\"full\" xml:space=\"preserve\" width=\"300px\" height=\"300px\" viewBox=\"0 0 300 300\">\n",
       "<!-- END OF HEADER -->\n",
       "<rect style=\"opacity:1.0;fill:#FFFFFF;stroke:none\" width=\"300.0\" height=\"300.0\" x=\"0.0\" y=\"0.0\"> </rect>\n",
       "<path class=\"bond-0 atom-0 atom-1\" d=\"M 15.0,146.3 L 51.5,144.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-1 atom-1 atom-2\" d=\"M 51.5,144.1 L 71.7,174.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-2 atom-2 atom-3\" d=\"M 71.7,174.6 L 108.2,172.3\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-3 atom-3 atom-4\" d=\"M 108.2,172.3 L 124.5,139.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-3 atom-3 atom-4\" d=\"M 114.5,172.0 L 128.0,144.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-4 atom-4 atom-5\" d=\"M 124.5,139.6 L 161.0,137.4\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-5 atom-5 atom-6\" d=\"M 161.0,137.4 L 181.2,167.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-5 atom-5 atom-6\" d=\"M 158.2,143.0 L 174.9,168.3\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-6 atom-6 atom-7\" d=\"M 181.2,167.9 L 164.9,200.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-7 atom-7 atom-8\" d=\"M 164.9,200.6 L 128.4,202.8\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-7 atom-7 atom-8\" d=\"M 161.4,195.3 L 131.2,197.2\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-8 atom-6 atom-9\" d=\"M 181.2,167.9 L 217.7,165.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-9 atom-9 atom-10\" d=\"M 217.7,165.6 L 234.0,132.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-10 atom-10 atom-11\" d=\"M 237.2,132.7 L 228.8,120.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-10 atom-10 atom-11\" d=\"M 228.8,120.1 L 220.5,107.5\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-10 atom-10 atom-11\" d=\"M 232.6,135.7 L 224.3,123.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-10 atom-10 atom-11\" d=\"M 224.3,123.1 L 215.9,110.5\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-11 atom-10 atom-12\" d=\"M 234.0,132.9 L 249.3,132.0\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-11 atom-10 atom-12\" d=\"M 249.3,132.0 L 264.6,131.0\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-12 atom-9 atom-13\" d=\"M 217.7,165.6 L 237.9,196.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-13 atom-1 atom-14\" d=\"M 51.5,144.1 L 67.8,111.4\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path class=\"bond-14 atom-8 atom-3\" d=\"M 128.4,202.8 L 108.2,172.3\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 70.7,173.1 L 71.7,174.6 L 73.5,174.5\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n",
       "<path d=\"M 123.7,141.2 L 124.5,139.6 L 126.3,139.5\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n",
       "<path d=\"M 159.2,137.5 L 161.0,137.4 L 162.0,138.9\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n",
       "<path d=\"M 165.7,199.0 L 164.9,200.6 L 163.1,200.7\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n",
       "<path d=\"M 130.2,202.7 L 128.4,202.8 L 127.4,201.3\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n",
       "<path d=\"M 233.2,134.5 L 234.0,132.9 L 234.8,132.8\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n",
       "<path class=\"atom-11\" d=\"M 209.1 102.4 Q 209.1 99.9, 210.3 98.5 Q 211.5 97.2, 213.8 97.2 Q 216.1 97.2, 217.4 98.5 Q 218.6 99.9, 218.6 102.4 Q 218.6 104.9, 217.3 106.4 Q 216.1 107.8, 213.8 107.8 Q 211.5 107.8, 210.3 106.4 Q 209.1 105.0, 209.1 102.4 M 213.8 106.6 Q 215.4 106.6, 216.3 105.6 Q 217.1 104.5, 217.1 102.4 Q 217.1 100.4, 216.3 99.4 Q 215.4 98.3, 213.8 98.3 Q 212.3 98.3, 211.4 99.4 Q 210.5 100.4, 210.5 102.4 Q 210.5 104.5, 211.4 105.6 Q 212.3 106.6, 213.8 106.6 \" fill=\"#FF0000\"/>\n",
       "<path class=\"atom-12\" d=\"M 265.8 130.7 Q 265.8 128.2, 267.0 126.8 Q 268.2 125.4, 270.5 125.4 Q 272.8 125.4, 274.1 126.8 Q 275.3 128.2, 275.3 130.7 Q 275.3 133.2, 274.0 134.6 Q 272.8 136.0, 270.5 136.0 Q 268.2 136.0, 267.0 134.6 Q 265.8 133.2, 265.8 130.7 M 270.5 134.9 Q 272.1 134.9, 273.0 133.8 Q 273.8 132.8, 273.8 130.7 Q 273.8 128.6, 273.0 127.6 Q 272.1 126.6, 270.5 126.6 Q 268.9 126.6, 268.1 127.6 Q 267.2 128.6, 267.2 130.7 Q 267.2 132.8, 268.1 133.8 Q 268.9 134.9, 270.5 134.9 \" fill=\"#FF0000\"/>\n",
       "<path class=\"atom-12\" d=\"M 276.9 125.5 L 278.3 125.5 L 278.3 129.9 L 283.6 129.9 L 283.6 125.5 L 285.0 125.5 L 285.0 135.9 L 283.6 135.9 L 283.6 131.1 L 278.3 131.1 L 278.3 135.9 L 276.9 135.9 L 276.9 125.5 \" fill=\"#FF0000\"/>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import safe as sf\n",
    "import datamol as dm\n",
    "\n",
    "ibuprofen_mol = dm.to_mol(ibuprofen)\n",
    "\n",
    "display(dm.to_image(ibuprofen_mol))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset:\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "train_dataset = load_dataset(\"datamol-io/safe-gpt\",split =\"train[:5%]\")\n",
    "eval_dataset = load_dataset(\"datamol-io/safe-gpt\",split =\"test[10%:15%]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc1ccc4cc1.C14(C)N=C(O)N5C1=O.C5C6=O.N67C.C78.C18CCC1\n"
     ]
    }
   ],
   "source": [
    "# print a random sample:\n",
    "print(eval_dataset[0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 27, 54, 14, 54, 54, 54, 17, 54, 54, 14, 11, 27, 14, 17, 7, 27, 8, 37, 23, 27, 7, 38, 8, 37, 18, 27, 14, 23, 38, 11, 27, 18, 27, 19, 23, 38, 11, 37, 19, 20, 27, 11, 27, 20, 21, 11, 27, 14, 21, 27, 27, 27, 14, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from safe.tokenizer import SAFETokenizer\n",
    "from safe.trainer.collator import SAFECollator\n",
    "\n",
    "tokenizer = SAFETokenizer().load(\"./tokenizer.json\")\n",
    "tokenizer = tokenizer.get_pretrained()\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "print(tokenizer(eval_dataset[0]['input']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\")\n",
    "\n",
    "# # Resize the model embeddings to match the new tokenizer\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.save_pretrained(\"phi1_5_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/safemol/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig\n",
    "from accelerate import init_empty_weights\n",
    "def get_device_map() -> str:\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device = get_device_map()  # 'cpu'\n",
    "\n",
    "model_id = \"microsoft/phi-1_5\"\n",
    "config = None\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "config.vocab_size = len(tokenizer)\n",
    "config.max_position_embeddings = 512\n",
    "\n",
    "config.bos_token_id = tokenizer.bos_token_id\n",
    "config.eos_token_id = tokenizer.eos_token_id\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "config.task_type = \"causal_lm\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,)\n",
    "with init_empty_weights():\n",
    "     model = AutoModelForCausalLM.from_pretrained(\"phi1_5_updated\", \n",
    "                                             config=config,\n",
    "                                             ignore_mismatched_sizes=True,\n",
    "                                             quantization_config=quantization_config, \n",
    "                                             device_map=\"auto\",\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             attn_implementation=\"flash_attention_2\"\n",
    "                                             )\n",
    "model.tie_weights()\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 1,220,925,272 || trainable%: 0.3865\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# We need to unfreeze the token embeddings because we changed the tokenizer\n",
    "\n",
    "embed_tokens_layer = model.base_model.model.model.embed_tokens\n",
    "# Unfreeze the parameters of embed_tokens\n",
    "for param in embed_tokens_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "# Optionally, verify that the parameters are unfrozen\n",
    "for name, param in model.named_parameters():\n",
    "    if 'embed_tokens' in name:\n",
    "        print(f\"{name} requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,568,832 || all params: 1,220,925,272 || trainable%: 0.7018\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SAFESplitter:\n",
    "    \"\"\"Standard Splitter for SAFE string\"\"\"\n",
    "\n",
    "    REGEX_PATTERN = r\"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n",
    "    name = \"safe\"\n",
    "\n",
    "    def __init__(self, pattern: Optional[str] = None):\n",
    "        if pattern is None:\n",
    "            pattern = self.REGEX_PATTERN\n",
    "        self.regex_pattern = pattern\n",
    "        self.regex = re.compile(pattern)\n",
    "\n",
    "    def tokenize(self, line):\n",
    "        \"\"\"Tokenize a safe string into characters.\"\"\"\n",
    "        if isinstance(line, str):\n",
    "            tokens = list(self.regex.findall(line))\n",
    "            reconstruction = \"\".join(tokens)\n",
    "            if line != reconstruction:\n",
    "                logger.error(\n",
    "                    f\"Tokens different from sample:\\ntokens {reconstruction}\\nsample {line}.\"\n",
    "                )\n",
    "                raise ValueError(line)\n",
    "        else:\n",
    "            idxs = re.finditer(self.regex, str(line))\n",
    "            tokens = [line[m.start(0) : m.end(0)] for m in idxs]\n",
    "        return tokens\n",
    "\n",
    "    def detokenize(self, chars):\n",
    "        \"\"\"Detokenize SAFE notation\"\"\"\n",
    "        if isinstance(chars, str):\n",
    "            chars = chars.split(\" \")\n",
    "        return \"\".join([x.strip() for x in chars])\n",
    "\n",
    "import datasets\n",
    "\n",
    "# Assume `train_dataset` and `eval_dataset` are your datasets\n",
    "\n",
    "# Initialize your custom splitter\n",
    "splitter = SAFESplitter()\n",
    "\n",
    "def custom_tokenize(example):\n",
    "    # Check if 'input' key exists and is not None\n",
    "    if 'input' in example and example['input'] is not None:\n",
    "        # Apply your custom tokenization logic\n",
    "        tokens = splitter.tokenize(example['input'])\n",
    "        # Join tokens with spaces (or any other suitable delimiter) to create a single string\n",
    "        return {\"input\": \" \".join(tokens)}\n",
    "    else:\n",
    "        # Return an empty dictionary to effectively skip this example\n",
    "        return {}\n",
    "\n",
    "def filter_empty_examples(example):\n",
    "    # Filter out empty examples\n",
    "    if 'input' in example and example['input']:\n",
    "        return True\n",
    "    else:\n",
    "        logger.info(f\"Filtering out empty example: {example}\")\n",
    "        return False\n",
    "\n",
    "# Apply the custom tokenization to the datasets\n",
    "train_dataset = train_dataset.map(custom_tokenize, batched=False, num_proc=32)\n",
    "eval_dataset = eval_dataset.map(custom_tokenize, batched=False, num_proc=32)\n",
    "\n",
    "# Filter out the empty examples\n",
    "train_dataset = train_dataset.filter(filter_empty_examples, num_proc=32)\n",
    "eval_dataset = eval_dataset.filter(filter_empty_examples, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import PreTokenizer, Whitespace\n",
    "\n",
    "tokenizer._tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/safemol/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 47,272,765\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 18\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 36\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,313,132\n",
      "  Number of trainable parameters = 8,568,832\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/thomas/anaconda3/envs/safemol/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
      "/home/thomas/anaconda3/envs/safemol/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='143' max='1313132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    143/1313132 03:55 < 609:08:52, 0.60 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/safemol/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to .saved_model/phi1_5-safmol/checkpoint-100\n",
      "tokenizer config file saved in .saved_model/phi1_5-safmol/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in .saved_model/phi1_5-safmol/checkpoint-100/special_tokens_map.json\n",
      "tokenizer config file saved in .saved_model/phi1_5-safmol/tokenizer_config.json\n",
      "Special tokens file saved in .saved_model/phi1_5-safmol/special_tokens_map.json\n",
      "/home/thomas/anaconda3/envs/safemol/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from safe.trainer.collator import SAFECollator\n",
    "\n",
    "trained_model_id = \"phi1_5-safmol\"\n",
    "\n",
    "output_dir = '.saved_model/' + trained_model_id\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=False, # specify bf16=True instead when training on GPUs that support bf16 else fp16\n",
    "    bf16=False,\n",
    "    tf32 = True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    optim = \"paged_adamw_8bit\" ,\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    eval_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=1, # originally set to 8\n",
    "    per_device_train_batch_size=18, # originally set to 8\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=trained_model_id,\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    report_to=\"none\",  # for skipping wandb logging\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_total_limit=None,\n",
    "    save_steps =100,\n",
    "    torch_compile = True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"input\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=False,\n",
    "        max_seq_length=512,\n",
    "        dataset_num_proc = 64,\n",
    "        data_collator=SAFECollator(tokenizer,input_key=\"input\",max_length=512)\n",
    "\n",
    "    )\n",
    "\n",
    "# To clear out cache for unsuccessful run\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_result = trainer.train()\n",
    "# resume_from_checkpoint=checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file phi1_5_updated/config.json\n",
      "Model config PhiConfig {\n",
      "  \"_name_or_path\": \"phi1_5_updated\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": null,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": null,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"phi\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"partial_rotary_factor\": 0.5,\n",
      "  \"qk_layernorm\": false,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1880\n",
      "}\n",
      "\n",
      "loading weights file phi1_5_updated/model.safetensors\n",
      "Generate config GenerationConfig {}\n",
      "\n",
      "All model checkpoint weights were used when initializing PhiForCausalLM.\n",
      "\n",
      "All the weights of PhiForCausalLM were initialized from the model checkpoint at phi1_5_updated.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n",
      "loading configuration file phi1_5_updated/generation_config.json\n",
      "Generate config GenerationConfig {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [N5P][Th][N3[Sc+2][Th]%1B%47[Br-]2+][Br-][Br-]g+][14[Th][N3[17F][Th][N5[Ni-4][Br-]2+][Br-][Br-][N5P][Th][N3B[Br-][Br-][N4P][Th][N5[Sc+2][Th]%1\n"
     ]
    },
    {
     "ename": "SAFEDecodeError",
     "evalue": "Failed to decode [N5P][Th][N3[Sc+2][Th]%1B%47[Br-]2+][Br-][Br-]g+][14[Th][N3[17F][Th][N5[Ni-4][Br-]2+][Br-][Br-][N5P][Th][N3B[Br-][Br-][N4P][Th][N5[Sc+2][Th]%1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/safemol/lib/python3.11/site-packages/safe/converter.py:434\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(safe_str, as_mol, canonical, fix, remove_added_hs, remove_dummies, ignore_errors)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[43msafe_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mas_mol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_mol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcanonical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_dummies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_dummies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_added_hs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_added_hs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/safemol/lib/python3.11/site-packages/safe/converter.py:187\u001b[0m, in \u001b[0;36mSAFEConverter.decoder\u001b[0;34m(self, inp, as_mol, canonical, fix, remove_dummies, remove_added_hs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical:\n\u001b[0;32m--> 187\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_smiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/safemol/lib/python3.11/site-packages/datamol/mol.py:409\u001b[0m, in \u001b[0;36mstandardize_smiles\u001b[0;34m(smiles)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mApply smile standardization procedure. This is a convenient function wrapped arrounf RDKit\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03msmiles standardizer and tautomeric canonicalization.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    standard_smiles: the standardized smiles\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m smiles \u001b[38;5;241m=\u001b[39m \u001b[43mrdMolStandardize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStandardizeSmiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m smiles\n",
      "\u001b[0;31mArgumentError\u001b[0m: Python argument types in\n    rdkit.Chem.MolStandardize.rdMolStandardize.StandardizeSmiles(NoneType)\ndid not match C++ signature:\n    StandardizeSmiles(std::string smiles)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSAFEDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_sequences[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_text)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMILES: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43msafe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m )\n",
      "File \u001b[0;32m~/anaconda3/envs/safemol/lib/python3.11/site-packages/safe/converter.py:446\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(safe_str, as_mol, canonical, fix, remove_added_hs, remove_dummies, ignore_errors)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ignore_errors:\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SAFEDecodeError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "\u001b[0;31mSAFEDecodeError\u001b[0m: Failed to decode [N5P][Th][N3[Sc+2][Th]%1B%47[Br-]2+][Br-][Br-]g+][14[Th][N3[17F][Th][N5[Ni-4][Br-]2+][Br-][Br-][N5P][Th][N3B[Br-][Br-][N4P][Th][N5[Sc+2][Th]%1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import safe\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_id = \"./.saved_model/phi1_5-safmol\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id+\"/checkpoint-100\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define generation parameters\n",
    "generation_args = {\n",
    "    \"max_length\": 50,  # Adjust the maximum length of the generated sequence\n",
    "    \"num_return_sequences\": 1,  # Number of sequences to generate\n",
    "    \"temperature\": 1.0,  # Sampling temperature\n",
    "    \"top_k\": 50,  # Top-k sampling\n",
    "    \"top_p\": 0.95,  # Top-p (nucleus) sampling\n",
    "    \"do_sample\": True,  # Use sampling instead of greedy decoding\n",
    "}\n",
    "\n",
    "# Generate a sequence without a prompt\n",
    "input_ids = torch.tensor([[tokenizer.pad_token_id]])  # Use the pad token ID as a dummy input\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_sequences = model.generate(input_ids, **generation_args)\n",
    "\n",
    "# Decode the generated sequence\n",
    "generated_text = tokenizer.decode(generated_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"SMILES: \", safe.decode(generated_text, canonical=True) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4727/4727 [00:00<00:00, 6848.09 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input lengths: [512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512]\n",
      "Mean tokenized input length: 512.0\n",
      "Max tokenized input length: 512\n",
      "Min tokenized input length: 512\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Set model_max_length to 512\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "# Define the fraction of the dataset to sample\n",
    "fraction = 0.0001  # 1% of the dataset\n",
    "\n",
    "# Sample a fraction of the dataset\n",
    "sample_size = int(len(train_dataset) * fraction)\n",
    "sample_indices = random.sample(range(len(train_dataset)), sample_size)\n",
    "sampled_dataset = train_dataset.select(sample_indices)\n",
    "\n",
    "# Tokenize the sampled dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['input'], truncation=True, padding='max_length', max_length=tokenizer.model_max_length)\n",
    "\n",
    "tokenized_dataset = sampled_dataset.map(tokenize_function, batched=True,num_proc=10)\n",
    "\n",
    "# Check the length of the tokenized inputs\n",
    "input_lengths = [len(input_ids) for input_ids in tokenized_dataset['input_ids']]\n",
    "print(\"Tokenized input lengths:\", input_lengths)\n",
    "\n",
    "# Optionally, print some statistics about the lengths\n",
    "import numpy as np\n",
    "\n",
    "print(\"Mean tokenized input length:\", np.mean(input_lengths))\n",
    "print(\"Max tokenized input length:\", np.max(input_lengths))\n",
    "print(\"Min tokenized input length:\", np.min(input_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safemol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
